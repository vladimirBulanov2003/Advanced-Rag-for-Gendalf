Вся логика программы описана в 4 файлах - streamlit_test.py, generate_documents.py, main.py, fill_database.py, extract_embeddings_from_metada.py

Алгоритм данного RAG следующий - производится извлечение метаданных из запроса пользователя, затем по ним, с помощью косинусной близости, находятся самые близкие метаданные, которые привязаны к нашим чанкам в датасете. Эти самые близкие метаданные будут использоваться как фильтр. Это позволяет нам извлечить общие сущности, которые фигурируют в текстах. После отбора самых релевантных метаданных и фильтрации по ним находятся чанки, содержащие те же самые обьекты, похожие действия, ошибки и тк. 

Затем среди этих отфильтрованных чанков находятся такие, чьи гипотетические вопросы максимально схожи с запросом пользователя в рамках той же косинусной близости, причем поиск осуществляется одновременно по всем 3 вопросам одновременно.
Таким образом, на данном этапе мы уже получаем вопросы, которые, максимально похожи с точки зрения метода гипотетического вопроса и с точки зрения наличия похожих обьектов, фигурирующих в запросе.

На последнем этапе мы находим документы, которые содержат эти самые "лучшие" чанки. Это было сделано для того, чтобы лучшие чанки были дополнены остальными из того же документа для избежания резкого прерывания потенциально нужной информации. В итоге, наш RAG прогоняется по тем чанкам, которые соержатся в документах с куском самой ценной и релевантной информацией.




Весь этап создания программы описан ниже, с небольшими разьяснениями по работе кода:

1) В generate_document было соврешено разбиение документов на чанки, а также сгенерированы метаданные и 3 гипотетических вопроса для каждого фрагмента с помощью pydantic. Все эти данные хранились в виде 2 словарей json на диске - один для документов, другой для метаданных.
Так же был использован asyncio для возможности праллельного обращения к OpenAI api.

2) После создания словарей было совершено преобразования текста в метаданных в эмбединги, данные действия совершались в файле extract_embeddings_from_metada.py. Для эмбедингов была использована модель "text-embedding-3-large". Полученные эмбединги метаданных будут в дальнейшем использоваться в поиске по косинусной близости для метаданных запроса пользователя. 

3) Совершена загрузка данных к вектороной базе данных qdrant с укзаанием индексов. Все было сделано в файле fill_database.py.

4) На этом этапе был написан код, позволяющий вычленить метеданные из запроса пользователя. Формирования промта с самыми релевантными чанкими обрзауется с помощью функции - extract_answer_from_llm_using_rag в фале main.py

5) На последнем этапе был создан сайт на streamlit, с возможностью запоминания истории чата с помощью langgraph.





